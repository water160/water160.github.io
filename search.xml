<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[聚类算法总结]]></title>
      <url>%2F2017%2F01%2F17%2FData-Mining-Clustering%2F</url>
      <content type="text"><![CDATA[作为“煤矿工作者”，聚类可以在迷茫的时候提供挖掘方向。嗯，Interesting~~ 划分方法 思想： 每个数据被归入相互不同重叠的 k 个 cluster 之一 目标： cluster 内距离最小 K-Means算法 算法思想： 指定 cluster 数目为 k； 随机划分数据到 k 个子集； 计算每个子集的“中心”数据； 【计算所有数据到 k 个“中心”距离】 【将每个数据所属类别调整到里数据最近“中心”所代表的 cluster/子集】 重复上述两个步骤，直至收敛。 算法优点： 简单，实现简单；运行时间复杂度较低: O(nkt)[ 元组数n × cluster数k × 迭代次数t)]。 目标明确：最小化类内距离。 算法不足： 易陷入局部最优解（和初始值密切相关）； “中心”计算时，标称数据不好处理； 需要预置k值； 对噪声数据/孤立点敏感； 非凸 cluster 的识别能力弱。 算法改进： K-Means 算法的“中心”点是虚拟数据，不一定在数据集合中存在，改成某实际靠近中心点且存在的数据，得到“k-中心点”算法； 降低了噪声、离群点的影响，增加了时间代价； 标称属性的“中心”用众数代替均值，及改进的距离计算方法； 改进初始时刻数据划分方法或中心点选择方法，如PAM算法。 PAM算法 算法思想： 核心思想：围绕中心点划分方法。 随机选择 k 个种子为中心点，即 cluster 的代表，将数据点划归到最近中心点/种子代表的 cluster； 对所有（种子，非种子）对，尝试交换它们，检查是否能提高聚类质量：所有元组到各自“中心”的距离和。选择最好的能提升结果质量所对应的交换，实施交换，直至算法收敛。 算法评述： K-medoids 算法的改进； 可以用一些启发式方法选择交换的种子和非种子； 易陷入局部最优。 针对大规模数据集改进算法 主要解决问题： 数据集无法一次载入内存；重复多次计算一个点/数据到其它数据的距离； CLARA算法： 对数据集中的数据进行采样，在采样得到的子集上寻找中心点，执行 PAM 算法； CLARANS算法： 执行 PAM 算法，其中没有搜索所有可能的实施交换的对，仅仅执行L次（种子，非种子）对的交换； 层次方法 层次聚类： 在不同概念层次上各自形成 clusters，构成一棵树状图(Dendrogram)重点考虑优化目标： cluster 之间的距离最大化核心问题： 两个 cluster 之间的距离如何计算的问题（采用最小、最大、平均距离、虚拟中心、Medoid距离等） 主要层次算法 AGNES算法（凝聚思想）： 自底向上，找两个簇，它们中最相似两个数据的距离最小，则合并这两个簇；迭代该过程，直至所有对象最终合并形成一个簇。 DIANA算法（分裂思想)： 自顶向下，找一个簇，簇中最近两个数据的距离在所有簇中最大，分裂该簇；迭代该过程； 计算量较大通常采用启发式，为了效率通常不对已做出的划分决策回溯； 算法评估： 聚合/分裂点的选择，极大影响算法的性能； 相对来说，算法时间复杂度较高； 针对大规模数据集改进算法BIRCH算法（利用层次结构的平衡迭代规约和聚类） 算法思想： 利用统计信息表示一个簇，避免存储簇中所有数据；实现簇信息的压缩存储。也可以认为是定义了一个簇的特征描述。 算法步骤： 扫描数据集，逐步构建一棵包括了簇统计信息 / 特征的树（聚类特征向量CF：包含聚类的特征，计算简单），非叶结点是统计信息；一个叶节点是一个簇，不同的数据归入不同簇； 若某个簇的数据太多，距离太大，考虑分裂该簇（及其父节点）； 算法特点： 每个非叶结点的聚类特征向量包括其所有“后代”叶子节点数据的统计信息； 新数据加入后，归入某一类，更新其所有父节点的统计信息/聚类特征向量； 当叶结点的父节点特征向量达到某个“临界点”/满足某些条件时，考虑分裂该节点； 增量式算法； 算法参数1：分支因子B，控制非叶结点的最大孩子数； 算法参数2：cluster直径最大值的界 T，超过就分裂此cluster。 算法优点： 时间复杂度O(n)，只需要装入一次数据集，适用于大型数据集； 重点在CF和CF-Tree的设计和利用很巧妙，压缩存储数据集； 动态调整参数，使得其和机器的内存配置相适应； 对叶条目对应的cluster可以继续处理：采用其它技术（比如聚类），删除小的cluster（离群点），合并稠密的cluster； 算法缺点： B，T和 L 的确定没有标准和经验，但它们显著影响性能； 对数据对象的输入次序敏感/次序相关的； T的使用，使得 cluster 形状受约束； Chameleon算法（使用动态建模的多阶段层次聚类） 核心思想： 两个cluster之间连边很多，且距离近，则合并之。 算法步骤： 从数据集开始，每个数据视为n-D空间中的一个点； 计算每个点的k个最近邻（构建KNN图），然后连接，并且每个连接赋予权值（距离）； 划分图：使得分割边（被切断的边）的权值之和最小，评估了簇之间的绝对互连性； 合并RI（相对互连度：(最小二分)边割和）和RC（相对接近度：(最小二分)平均权重）大于用户指定阈值的簇。 时间复杂度最坏是O(n^2)； 概率层次聚类算法 上述层次聚类算法的缺点： 为层次聚类选择一种好的距离度量常常是困难的； 为了使用算法，数据对象不能有缺失的属性值，而实际中数据被部分观测情况下很难计算距离； 大部分算法的层次聚类方法都是启发式的（每一步局部搜索号的合并/划分），因此聚类层次结果的优化目标可能不清晰； 针对上述层次聚类算法的改进： 使用概率模型度量簇之间的距离，使用生成模型，找出拟合观测数据的最佳参数值） 算法核心思想： 用概率模型来度量 cluster 之间的距离，避免层次方法中最优距离定义问题； 基于生成模型（一个联合概率分布），例如每个 cluster 来自一个高维高斯分布等；cluster 是该生成模型的一个样本集；生成模型的参数由 cluster 的样本集估计出来；生成模型将对样本集进行新的划分； 该过程循环迭代直到收敛； 算法基本过程就是 EM 或 K-Means。 基于密度的方法DBSCAN算法（一个cluster就是密度连通的最大点集） 一些概念： q [直接密度可达] p：q 是核心对象，q 的领域中有 p； q [密度可达] p：存在一条 q 到 p 的链路，前一个数据对象直接可达后一个数据对象； 链上前n-1个数据对象都是核心对象； 对直接密度可达概念的扩展； p, q密度连通：任意给定两个数据对象 p, q，当且仅当存在一个数据 o，o 分别密度可达 p, q。 算法步骤： 初始时刻，所有点都标记为unvisited，随机选择一个记为 p，并标记为 visited，检查是否是核心对象，否则记为噪声/非核心点； 若 p 是核心对象，则创建新簇C，将Nε(p)（p的邻域点集）添加到集合 N；检查 N 中的点，假设为 q，加入 C（必然是p簇中的一个），检查 q 的标记，若是 unvisited，则改为 visited，并检查其邻域大小|Nε(q)|,若 q 为核心对象，则Nε(q)（q的邻域点集）都加入 N； N 为空时，找到一个簇 C； 重复上述过程，得到其它的簇。 算法评估： 使用空间索引可以使得时间复杂度为 O(nlogn)，n 为数据库对象数，其复杂度为O(n^2)； 若 ε和 Minpts 设置恰当，则该算法可以有效地发现任意形状的簇； 但是参数的设置难以确定，特别对于高维数据集，对参数值敏感； 只有一组全局参数，使得基于密度的簇关于邻域的阈值是单调的。 OPTICS算法（使用簇排序，不显式产生数据集聚类） 算法假设： 任何一个点的概率密度依赖该点（出现概率较大）到样本的距离； 一些概念： 核心距离：p 核心距离是最小半径ε’，使得至少包含Minpts个对象，若 p 不是核心对象，则没有定义。 可达距离：q 到 p 的可达距离是从 q 密度可达 p 的最小半径值（q 必须是核心对象，p 必须在 q 的邻域内），可达距离为 max{ core-distance(q), dist(p, q) }，若 q 不是核心对象，则没有定义。 算法步骤： 开始用数据库中的任意对象作为当前对象 p，检索 p 的 ε 邻域，确定核心距离并设置可达距离为未定义 输出当前对象 p，若 p 不是核心对象，则转移到 OrderSeeds表下一个（若OrderSeeds表为空，则找数据库中的下一个）； 若 p 是核心对象，则对于 p 的 ε 邻域中的每个对象 q，更新从 p 到 q 的可达距离，并且如果 q 尚未处理则加入到OrderSeeds表中 不断迭代知道输入数据库为空，并且OrderSeeds为空。 算法评估： 算法结构与DBSCAN相似，使用空间索引可以使得时间复杂度为O(nlogn)，否则为O(n^2)。 DBSCAN 和 OPTICS 算法的密度估计方式对半径值非常敏感，随着半径的稍微增加，密度显著改变，需要使用核密度估计（非参数密度估计方法，减少参数值带来的敏感性）。 DENCLUE算法（基于密度分布函数的聚类） 核心思想： 想象一个场/平面，上面堆了很多沙堆，每个沙堆独立看，是“正态分布”； 平面上的每个点都是很多个沙堆“叠加”出来一个高度，高度代表密度，出现一个新样本的概率； “正态”沙堆多了，彼此交叠，就成了“绵延起伏”的沙漠；沙漠中存在一些沙丘的高点，高于周围的沙；形成一个局部最大值；正式的名称是局部吸引子 达到一定高度 h 的局部吸引子可以被视为簇中心，用来把样本数据划归到不同的簇中。 用爬山式方法把样本划分给局部吸引子，因为“沙漠”是一个概率密度函数，存在梯度，沿梯度方向让每个样本找到一个“山顶”。 若一个样本数据爬到了一个非局部吸引子的山顶（&lt; h），那么这个数据就被认为“噪声”; 算法评估： DBSCAN 的一般化，抗噪声。核密度估计通过把噪声均匀分不到输入数据，降低噪声影响。 基于网格的方法STING（统计信息网格网格） 方法介绍： 空间被划分为多个超立方体的小格子，小格子具有不同的层次/分辨率，高层小格子被划分为多个低层的小格子； 每个小格子有统计信息（计数 / 最大值 / 最小值 / 均值 / 标准差 / 服从分布等）； 高层格子的统计信息可由底层的来计算； 方法评估： 基于网格的计算独立于查询，不依赖于查询（因为存储在每个单元的统计信息提供了数据汇总信息）； 网格结构利于并行处理与增量更新； 主要优点：效率高（产生聚类的时间复杂度O(n)，n是对象数；层次结构建立后，查询时间为O(g)，g是最底层网格单元个数，通常远小于n）； 主要缺点：由于未考虑子女单元和其相邻单元之间的联系，分类边界都是水平或垂直的线，可能降低簇的质量和精确性。 CLIQUE（类Apriori基于网格的聚类，发现子空间基于密度的簇） 核心思想： 如果在3-D空间中是稠密的，那么在其任何投影空间中都应该是稠密的，类似Apriori思想，寻找高维空间的稠密单元。 算法步骤： 第一阶段：把d-维数据空间划分若干互不重叠的矩形单元，并且从中识别出稠密单元，迭代连接子空间，并检查连接后的子空间中的点数是否满足密度阈值，直到都不稠密时终止。 第二阶段：使用每个子空间中的稠密单元来装配可能具有任意形状的簇（利用最小描述长度，使用最大区域来覆盖连接的稠密单元。原问题是 NP 难的，因此采用贪心策略）。 方法评估： 自动识别高维数据空间中数据稠密的子空间； 不需要对数据分布做假设，数据输入次序无关/不敏感； 输入数据的多少和维度，对算法的时间复杂度影响是线性的； 缺点：聚类精度可能会降低。 聚类评估聚类评估主要步骤（三点） 聚类开始前： 估计聚类趋势（Hopkins统计量），判断能否聚类[均匀数据集不可以聚类]。 判断可以聚类之后： 确定簇数（经验方法、肘方法、交叉验证法）； 聚类完成后： 度量聚类质量（给定标准→外在方法；没有标准→内在方法）； 估计聚类趋势（步骤一） 每个均匀采样得到的样本都找一个离自己最近的数据集中的点，记为xi； 均匀随机从数据集 D 中采样 n 个点 q1, q2, … , qn,并找出数据集 D 中离 qi 最近（除了 qi 本身以外 ）的点，记为yi； Hopkins统计量： 包含簇（距离较近）时，∑ yi 会显著小于∑ xi ，H接近0； 不包含明显的“自然簇”时（距离较远），H约为0.5。 确定簇数（步骤二） 1. 经验方法：√(n/2)个簇，每个簇√(2n)个数据，总数据n个 2. 肘方法 核心思想：增加簇数 k，检查簇内方差 S 随着 k 降低的速度，找拐点： 给定 k，采用一种聚类方法对数据集聚类，计算每个簇的簇内方差之和 S； 增加 k，S 会降低，因为簇多了，簇小了，簇内差异变小； 检查 S 随 k 降低的速度，或者说找第一个“拐点”； 3. 交叉验证法： 将数据集划分为近似相等的 m 分； 用其中 m-1 份构建聚类模型，剩下一份检验聚类质量（如：数据到最近“簇心”的距离平方和）； 对给定的 k，重复上述过程 m 次； 比较不同的 k； 度量聚类质量（步骤三） 外在方法（参考混淆矩阵）： 给定一个标准，标准看成是“类标号”，故是有监督的（Bcubed精度和召回率）； 准确率： 一个对象的精度指：C 给出的同一个簇中，多少个等于 L 给定的 一个对象的召回率指：L 给出的同一个簇的对象多少被 C 正确给定 内在方法： a(o)是数据 o 到其所属簇的其它数据的距离均值，表示簇内紧凑性； b(o)是数据 o 到其它数据对象的距离均值，表示 o 和其它簇的簇间分离性； 度量标准——轮廓系数： s(o)取值在[-1, 1]，接近1时，o所在簇紧凑且远离其它簇； s(o)为负数时，表示o离其它簇对象的距离比自己所在簇的对象更近； 如何评估聚类方法获得的聚类质量? 所有对象的 轮廓系数之和/均值 。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2016 Machine-Learning FINAL EXAM]]></title>
      <url>%2F2017%2F01%2F17%2FMachine-Learning-Final-Exam%2F</url>
      <content type="text"><![CDATA[The generation of random numbers is too important to be left to chance, so did the exam. 2016年机器学习与知识发现期末试卷（回忆版） 一、训练误差和测试误差随模型复杂度变化的曲线，什么是奥卡姆剃刀原理？ 二、给定样本联合概率分布表格，利用朴素贝叶斯估算某个测试样本点的类标号。 三、信息增益决策树计算并选择分类属性？决策树和朴素贝叶斯分别如何处理连续属性值？决策树只有三个属性，每个属性两个取值，类标号有两种，问最多有多少种不同的决策树？ 四、给定正例点和负例点，计算分离超平面和支持向量。 五、给定事务表格，问有多少种关联规则（支持度为0的也可以）？问最长的项集有哪些（支持度 &gt; 0）？问最频繁的项集是哪些（项集数大于等于2）？ 六、集成学习中为什么随即森林算法效率比Bagging更高？还有哪些多样性学习的算法，给出使用性场景。 七、领导者算法（会给出一些算法的基本步骤，大概类似于K中心点）同K-Means算法比较有什么优点和缺点？利用相似性矩阵进行单链和全链层次聚类，画出相应的图。 八、PCA主成分分析，为什么舍弃一部分信息是必要的？如何针对非线性情况进行降维？ 九、给定天猫用户和商家的用户线上行为、用户线下行为、商家信息（一张表格）来挖掘用户是否会购买商品和商家的关系。给出预测算法？相应的方案？哪些数据会影响预测结果？ PS： 题量和计算量都很大，估计好时间； 有些考到的概念能在《机器学习》书上相应章节找到； 作业题会有变化，一定要看清楚题目； 尽量先把会做的题做上，那些不太看得懂的题也要写上，不要空着；]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2016 Data-Mining FINAL EXAM]]></title>
      <url>%2F2017%2F01%2F17%2FData-Mining-Final%20Exam%2F</url>
      <content type="text"><![CDATA[If the answers on the draft and the paper disagree, then both are probably wrong. 2016年数据仓库与数据挖掘期末试卷（回忆版）一、填空题（50分） 五数概括（注意：卡方检验，Pearson系数，协方差） 最小-最大规范化、小数规范化（注意：z-分数规范化） 闵可夫斯基距离的表现形式（p=1，p=2，p=+∞） 关联规则的计算（给定支持度计数、关联规则，填空支持度和置信度） 贝叶斯网络的约简表示，一个五维向量（每个向量有3种值），4条边，若不采用贝叶斯网络则需要多少行(243)？问最少(39)、最多要存储多少行(255)? n个变量，每个变量两个值，y = f (x1, x2, x3, …, xn)，y∈{0, 1, 2}，问有f有多少种不同的函数？假设有m个样本(m &lt; 2n)，则有多少不同的函数，若采用最小描述长度表示，则有多少个参数？（不懂……） 提升度lift计算公式，保留（大于）1的时候的频繁项集。（注意：混淆矩阵、准确率、精度、召回率、F分数，上述会结合KNN先做分类，然后画出混淆矩阵） f近似函数h，评估h优劣的准则有哪三个（准确性、复杂性、完备性）？由于计算复杂性不能再所有数据上进行计算，则可以在什么上面计算来评估h（测试数据集）？ 二、贝叶斯网络（10分） 考察贝叶斯网络中联合概率分布的形式P(U, W, X, Y, Z) = P(U) · P(W | X) · P(X | Y, Z)……类似这种形式。 如何用SQL语句得到某个条件概率P(U=u,W=w|X=x,Y=y)的值，给出计算过程。（注意：边缘概率、条件概率、联合概率分布表的存储行数计算） 三、频繁项集挖掘算法（11分） 对于给定的事务表格和支持度，利用Apriori算法进行频繁项集挖掘，写出主要步骤过程（不用伪代码表示过程） 给出一个闭频繁模式、一个极大频繁模式和一个关联规则（注意：如果只给定一棵FP树，要求你给出频繁项集和关联规则，指定支持度和置信度） 四、分类问题（14分） 对于给定数据集D=(xi,yi),i=1,2,…,|D|，yi∈{+1,-1}： 假设用线性函数h来做分类，考虑正则化，请给出该优化问题的形式化描述 假设采用朴素贝叶斯分类器，画出网络结构，并给出相应的训练/学习算法（注意：如何利用决策树分类或贝叶斯网填补表格缺失值？） 五、聚类问题（15分） 给定8个数据点的二维坐标：(0,1), (2,4), (2,1), (1,3), (4,3), (4,4), (6,3), (6,5)。 假设k=2，中心点为(0,1)和(2,4)，使用k中心点算法聚类，给出前三次迭代的过程。 聚类完成后，数据点(4,3)的轮廓系数 若Minpts=2，ε=2，这些数据点中有哪些是核心对象（3个）？]]></content>
    </entry>

    
  
  
</search>
